# -*- coding: utf-8 -*-
"""RAGFULLCLASS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WJXMoZ9sYAOh0BcchhnpN2tWeB47M6Tj
"""

import re
import json
import datetime
from typing import List, Dict, Any, Optional, Tuple

import torch
import torch.nn.functional as F
import requests
from requests import Session
from openai import OpenAI
import google.generativeai as genai

class RAGSystem:
    """
    A single-class implementation of a full RAG pipeline including:
    - Markdown table chunking
    - Text embedding via NVIDIA API
    - GPU-accelerated retrieval with PyTorch
    - Reranking via NVIDIA API
    - Response generation via Gemini
    """

    # --- Constants ---
    DEFAULT_EMBED_MODEL = "nvidia/llama-3.2-nemoretriever-300m-embed-v1"
    DEFAULT_EMBED_URL = "https://integrate.api.nvidia.com/v1"

    DEFAULT_RERANKER_MODEL = "nvidia/llama-3.2-nemoretriever-500m-rerank-v2"
    DEFAULT_RERANKER_URL = "https://ai.api.nvidia.com/v1/retrieval/nvidia/llama-3_2-nemoretriever-500m-rerank-v2/reranking"

    DEFAULT_GENERATOR_MODEL = 'gemini-2.5-flash'

    def __init__(
        self,
        embed_api_key: str,
        rerank_api_key: str,
        gemini_api_key: str,
        embed_model: str = DEFAULT_EMBED_MODEL,
        embed_base_url: str = DEFAULT_EMBED_URL,
        reranker_model: str = DEFAULT_RERANKER_MODEL,
        reranker_url: str = DEFAULT_RERANKER_URL,
        generator_model: str = DEFAULT_GENERATOR_MODEL,
        device: Optional[str] = None
    ):
        """
        Initializes the entire RAG pipeline system.

        Args:
            embed_api_key: NVIDIA API key for the embedding service.
            rerank_api_key: NVIDIA API key for the reranking service.
            gemini_api_key: Google API key for the Gemini generation service.
            embed_model: The embedding model to use.
            embed_base_url: The base URL for the embedding API.
            reranker_model: The reranker model to use.
            reranker_url: The API endpoint for the reranker.
            generator_model: The Gemini model to use for generation.
            device: PyTorch device ('cuda', 'cpu', or None for auto-detect).
        """

        # --- Device Setup ---
        if device is None:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(device)
        print(f"RAGSystem initialized. Using device: {self.device}")

        # --- API Keys ---
        self.embed_api_key = embed_api_key
        self.rerank_api_key = rerank_api_key
        self.gemini_api_key = gemini_api_key

        # --- Model Configs ---
        self.embed_model = embed_model
        self.reranker_model = reranker_model
        self.reranker_url = reranker_url
        self.generator_model = generator_model

        # --- API Clients ---
        # 1. Embedding Client (OpenAI-compatible)
        self.embed_client = OpenAI(api_key=self.embed_api_key, base_url=embed_base_url)

        # 2. Reranker Client (Requests Session)
        self.reranker_session = requests.Session()
        self.reranker_session.headers.update({
            "Authorization": f"Bearer {self.rerank_api_key}",
            "Accept": "application/json",
        })

        # 3. Generator Client (Gemini)
        try:
            genai.configure(api_key=self.gemini_api_key)
            self.generator_client = genai.GenerativeModel(self.generator_model)
            print(f"Gemini client configured with model: {self.generator_model}")
        except Exception as e:
            print(f"Warning: Failed to configure Gemini client: {e}")
            self.generator_client = None

        # --- Data Storage ---
        self.document_embeddings: Optional[torch.Tensor] = None
        self.chunks_metadata: List[Dict[str, Any]] = []

    def __del__(self):
        """Clean up resources, like the requests session."""
        print("Closing RAGSystem resources...")
        if hasattr(self, 'reranker_session') and self.reranker_session:
            self.reranker_session.close()

    # --------------------------------------------------------------------------
    # 1. CHUNKING
    # --------------------------------------------------------------------------

    @staticmethod
    def chunk_by_table_row(markdown_content: str, min_char_len: int = 10) -> list[str]:
        """
        Splits the markdown content into a list of table row strings.

        It identifies lines that look like markdown table rows (start and end with '|')
        and actively filters out the table separator line (e.g., |---|---|).

        Args:
            markdown_content: The full text content of the markdown file.
            min_char_len: The minimum character length for a row to be included.

        Returns:
            A list of strings, where each string is a single table row.
        """
        # Regex to detect the separator line, e.g., |---| or |:---| or | --- |
        # This matches lines containing *only* pipe, hyphen, colon, and whitespace
        separator_regex = re.compile(r'^[|\s:-]+$')

        lines = markdown_content.split('\n')
        table_rows = []

        for line in lines:
            cleaned_line = line.strip()

            # Check if it looks like a table row (starts and ends with a pipe)
            if cleaned_line.startswith('|') and cleaned_line.endswith('|'):

                # Check if it's the separator line. If so, skip it.
                if separator_regex.match(cleaned_line):
                    continue

                # Only add non-empty rows with at least the minimum characters
                if len(cleaned_line) >= min_char_len:
                    table_rows.append(cleaned_line)

        return table_rows

    # --------------------------------------------------------------------------
    # 2. EMBEDDING & INDEXING (Internal Helpers + Public Method)
    # --------------------------------------------------------------------------

    def _embed_single_chunk(self, text: str, input_type: str = "query") -> Optional[List[float]]:
        """Internal: Generate embedding for a single text chunk."""
        try:
            response = self.embed_client.embeddings.create(
                input=[text],
                model=self.embed_model,
                encoding_format="float",
                extra_body={"input_type": input_type, "truncate": "NONE"}
            )
            return response.data[0].embedding
        except Exception as e:
            print(f"Error embedding chunk: {e}")
            return None

    def _embed_query(self, query: str) -> Optional[torch.Tensor]:
        """Internal: Generate and tensorize embedding for a user query."""
        print(f"Generating embedding for query: '{query[:100]}...'")
        embedding = self._embed_single_chunk(query, input_type="query")
        if embedding:
            print(f"Successfully generated query embedding (dimension: {len(embedding)})")
            return torch.tensor(embedding, dtype=torch.float32, device=self.device)
        else:
            print("Failed to generate query embedding")
            return None

    def _embed_multiple_chunks(
        self,
        chunks: List[str],
        input_type: str = "passage",
        batch_size: int = 10,
        show_progress: bool = True
    ) -> Optional[torch.Tensor]:
        """Internal: Generate embeddings for multiple text chunks in batches."""
        embeddings = []
        total_batches = (len(chunks) + batch_size - 1) // batch_size

        for i in range(0, len(chunks), batch_size):
            batch = chunks[i:i + batch_size]
            batch_num = i // batch_size + 1

            if show_progress:
                print(f"Processing embedding batch {batch_num}/{total_batches}...")

            try:
                response = self.embed_client.embeddings.create(
                    input=batch,
                    model=self.embed_model,
                    encoding_format="float",
                    extra_body={"input_type": input_type, "truncate": "NONE"}
                )
                for data in response.data:
                    embeddings.append(data.embedding)
            except Exception as e:
                print(f"Error in batch {batch_num}: {e}. Skipping batch.")
                continue

        if not embeddings:
            print("No embeddings were generated.")
            return None

        # Convert to PyTorch tensor and move to the designated device
        embedding_tensor = torch.tensor(embeddings, dtype=torch.float32, device=self.device)
        return embedding_tensor

    def index_documents(
        self,
        chunks: List[str],
        batch_size: int = 32,
        show_progress: bool = True
    ) -> None:
        """
        Public: Index documents by generating and storing their embeddings.

        Args:
            chunks: List of text chunks to index.
            batch_size: Batch size for embedding generation.
            show_progress: Whether to show progress.
        """
        print(f"Indexing {len(chunks)} documents...")

        # Generate embeddings
        self.document_embeddings = self._embed_multiple_chunks(
            chunks,
            input_type="passage",
            batch_size=batch_size,
            show_progress=show_progress
        )

        if self.document_embeddings is None:
            print("Indexing failed as no embeddings were generated.")
            self.chunks_metadata = []
            return

        # Store metadata
        self.chunks_metadata = [{"id": i, "text": text} for i, text in enumerate(chunks)]

        print(f"Successfully indexed {len(self.chunks_metadata)} documents")
        print(f"Embeddings tensor shape: {self.document_embeddings.shape}")
        print(f"Embeddings stored on: {self.document_embeddings.device}")

    # --------------------------------------------------------------------------
    # 3. RETRIEVAL (Internal Helpers + Public Method)
    # --------------------------------------------------------------------------

    @staticmethod
    def _batch_calculate_similarities(
        query_embedding: torch.Tensor,
        document_embeddings: torch.Tensor,
        normalize: bool = True
    ) -> torch.Tensor:
        """Internal: Calculate cosine similarities efficiently using PyTorch."""
        if query_embedding.dim() == 1:
            query_embedding = query_embedding.unsqueeze(0) # [1, dim]

        if normalize:
            query_norm = F.normalize(query_embedding, p=2, dim=1)
            doc_norm = F.normalize(document_embeddings, p=2, dim=1)
        else:
            query_norm = query_embedding
            doc_norm = document_embeddings

        # [1, dim] x [dim, num_docs] = [1, num_docs]
        similarities = torch.mm(query_norm, doc_norm.t()).squeeze(0)
        return similarities

    @staticmethod
    def _find_top_k_similar(
        query_embedding: torch.Tensor,
        document_embeddings: torch.Tensor,
        top_k: int = 5,
        threshold: Optional[float] = None
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """Internal: Find top-k most similar documents using PyTorch."""
        similarities = RAGSystem._batch_calculate_similarities(query_embedding, document_embeddings)

        if threshold is not None:
            mask = similarities >= threshold
            # Set scores below threshold to -inf
            similarities[~mask] = float('-inf')

        # Get top-k using PyTorch's topk
        top_k = min(top_k, len(similarities))
        scores, indices = torch.topk(similarities, k=top_k, largest=True, sorted=True)

        # Filter out -inf values if threshold was applied
        if threshold is not None:
            valid_mask = scores > float('-inf')
            scores = scores[valid_mask]
            indices = indices[valid_mask]

        return indices, scores

    def retrieve(
        self,
        query: str,
        top_k: int = 10,
        threshold: Optional[float] = None,
        debug_json_path: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        Public: Search for documents similar to the query.

        Args:
            query: Search query.
            top_k: Number of results to return.
            threshold: Optional similarity threshold.
            debug_json_path: If provided, saves the raw retrieval results
                             to this JSON file path.

        Returns:
            List of search results with scores.
        """
        if self.document_embeddings is None:
            raise ValueError("No documents indexed. Call index_documents first.")

        # 1. Generate query embedding
        query_embedding = self._embed_query(query)
        if query_embedding is None:
            return []

        # 2. Find top-k similar documents
        indices, scores = self._find_top_k_similar(
            query_embedding,
            self.document_embeddings,
            top_k=top_k,
            threshold=threshold
        )

        # 3. Format results
        results = []
        indices_cpu = indices.cpu().numpy()
        scores_cpu = scores.cpu().numpy()

        for idx, score in zip(indices_cpu, scores_cpu):
            chunk_meta = self.chunks_metadata[idx]
            results.append({
                "chunk_id": chunk_meta["id"],
                "similarity_score": float(score),
                "text": chunk_meta["text"],
                "text_preview": chunk_meta["text"][:200] + "..."
            })

        print(f"Retrieved {len(results)} chunks for query.")

        # 4. **NEW**: Save to debug JSON file if path is provided
        if debug_json_path:
            try:
                debug_data = {
                    'query': query,
                    'retrieved_at': datetime.datetime.now().isoformat(),
                    'retrieval_top_k': top_k,
                    'retrieval_threshold': threshold,
                    'results': results
                }
                with open(debug_json_path, 'w', encoding='utf-8') as f:
                    json.dump(debug_data, f, indent=2, ensure_ascii=False)
                print(f"Debug retrieval results saved to {debug_json_path}")
            except Exception as e:
                print(f"Warning: Failed to save debug JSON: {e}")

        return results

    # --------------------------------------------------------------------------
    # 4. RERANKING
    # --------------------------------------------------------------------------

    def rerank(
        self,
        query: str,
        search_results: List[Dict[str, Any]],
        top_k: Optional[int] = None
    ) -> List[Dict[str, Any]]:
        """
        Public: Reranks a list of search results using the NVIDIA Reranker API.

        Args:
            query: The original user query.
            search_results: The list of dicts from the retrieve() method.
            top_k: If provided, truncates the reranked list to this size.

        Returns:
            A new list of the search results, sorted by the new 'rerank_score'.
        """
        if not search_results:
            print("No search results to rerank.")
            return []

        print(f"Reranking {len(search_results)} results for query: '{query[:50]}...'")

        # Format passages as required by the API
        passages_for_api = [{"text": res["text"]} for res in search_results]

        payload = {
            "model": self.reranker_model,
            "query": {"text": query},
            "passages": passages_for_api
        }

        try:
            response = self.reranker_session.post(self.reranker_url, json=payload)
            response.raise_for_status() # Raise an error for bad responses
            response_body = response.json()

            rankings = response_body.get("rankings")
            if not rankings:
                print("Warning: Reranker response did not contain 'rankings'. Returning original results.")
                return search_results

            # Create a copy to avoid modifying the original list
            reranked_data = list(search_results)

            # Map rankings back to our original results
            for rank_data in rankings:
                original_index = rank_data.get("index")
                score = rank_data.get("logit") # The reranker score

                if original_index is None or score is None:
                    continue

                if 0 <= original_index < len(reranked_data):
                    reranked_data[original_index]["rerank_score"] = score

            # Sort the list by the new rerank score, highest first
            reranked_data.sort(key=lambda x: x.get("rerank_score", float('-inf')), reverse=True)

            print("Reranking complete.")

            # Truncate to top_k if specified
            if top_k is not None:
                reranked_data = reranked_data[:top_k]
                print(f"Returning reranked top {len(reranked_data)} results.")

            return reranked_data

        except requests.RequestException as e:
            print(f"Error calling reranker API: {e}. Returning original (non-reranked) results.")
            return search_results
        except Exception as e:
            print(f"Error processing reranker response: {e}. Returning original (non-reranked) results.")
            return search_results

    # --------------------------------------------------------------------------
    # 5. GENERATION
    # --------------------------------------------------------------------------

    def generate(
        self,
        query: str,
        retrieved_chunks: List[Dict[str, Any]],
        top_k_context: int = 5
    ) -> str:
        """
        Public: Generates a response using the Gemini API based on retrieved chunks.

        Args:
            query: The original user query.
            retrieved_chunks: A list of dictionaries, assumed to be pre-sorted.
            top_k_context: The number of top chunks to use as context.

        Returns:
            A string containing the generated answer.
        """
        if not self.generator_client:
            return "Error: Gemini client is not configured. Please check your API key."

        # 1. Extract text from the top-k chunks
        top_chunks = retrieved_chunks[:top_k_context]
        context_list = [chunk['text'] for chunk in top_chunks]

        if not context_list:
            return "Sorry, no relevant context was found to answer your query."

        # 2. Format the context for the prompt
        context_string = "\n\n---\n\n".join(context_list)

        # 3. Create the prompt
        prompt = f"""
        You are a helpful assistant. Please answer the following query based *only* on the provided context.
        If the answer cannot be found in the context, state that you cannot answer the question with the information given.

        QUERY:
        {query}

        CONTEXT:
        {context_string}

        ANSWER:
        """

        # 4. Generate the response
        print(f"Generating response for query: '{query[:50]}...'")
        try:
            response = self.generator_client.generate_content(prompt)
            return response.text
        except Exception as e:
            return f"Error generating response from Gemini: {e}"

    # --------------------------------------------------------------------------
    # 6. FULL PIPELINE
    # --------------------------------------------------------------------------

    def run_pipeline(
        self,
        query: str,
        retrieve_top_k: int = 20,
        rerank_top_k: int = 5,
        generate_context_top_k: int = 5,
        debug_json_path: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Runs the full RAG pipeline: Retrieve -> Rerank -> Generate.

        Args:
            query: The user's query.
            retrieve_top_k: How many documents to fetch in the initial retrieval.
                            (Should be larger than rerank_top_k).
            rerank_top_k: The number of documents to keep after reranking.
            generate_context_top_k: The number of reranked documents to pass
                                    to the generator.
            debug_json_path: Path to save the raw *retrieval* (step 1) results.

        Returns:
            A dictionary containing the query, final answer, and intermediate results.
        """
        print(f"\n{'='*60}\nRunning RAG pipeline for query: '{query}'\n{'='*60}")

        # --- 1. RETRIEVE ---
        print(f"\n--- Step 1: Retrieving (Top {retrieve_top_k}) ---")
        try:
            retrieved_results = self.retrieve(
                query,
                top_k=retrieve_top_k,
                debug_json_path=debug_json_path
            )
        except Exception as e:
            print(f"Error during retrieval: {e}")
            return {"query": query, "error": str(e)}

        if not retrieved_results:
            print("No results found during retrieval. Pipeline stopped.")
            return {
                "query": query,
                "answer": "Sorry, I could not find any relevant information to answer your query.",
                "retrieved_results": [],
                "reranked_results": []
            }

        # --- 2. RERANK ---
        print(f"\n--- Step 2: Reranking (Top {rerank_top_k}) ---")
        reranked_results = self.rerank(
            query,
            retrieved_results,
            top_k=rerank_top_k
        )

        # --- 3. GENERATE ---
        print(f"\n--- Step 3: Generating (Context {generate_context_top_k}) ---")
        final_answer = self.generate(
            query,
            reranked_results, # Pass the reranked results
            top_k_context=generate_context_top_k
        )

        print(f"\n{'='*60}\nPipeline Complete\n{'='*60}")

        # --- 4. RETURN ---
        return {
            "query": query,
            "answer": final_answer,
            "reranked_results": reranked_results,
            "retrieved_results": retrieved_results, # Original, pre-reranked results
        }


# =============================================================================
# EXAMPLE USAGE
# =============================================================================

if __name__ == "__main__":

    # --- 1. Configuration ---
    # !!! REPLACE WITH YOUR API KEYS !!!
    NVIDIA_EMBED_API_KEY = "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"
    NVIDIA_RERANK_API_KEY = "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"
    GOOGLE_GEMINI_API_KEY = "AIzaSyAylPLQObY6hI7b5ZKsGGSZjCzusH5YzaA" # Your Gemini API Key

    # Input file (adjust path as needed)
    INPUT_FILE = "/content/1-s2.0-S0141029696001496-main.md"

    # Check if keys are set
    if "nvapi-" not in NVIDIA_EMBED_API_KEY or "nvapi-" not in NVIDIA_RERANK_API_KEY or "AIza" not in GOOGLE_GEMINI_API_KEY:
        print("="*60)
        print("WARNING: API keys not set.")
        print("Please set your API keys at the top of the 'if __name__ == \"__main__\":' block.")
        print("="*60)
    else:
        try:
            # --- 2. Initialize the RAG System ---
            rag_system = RAGSystem(
                embed_api_key=NVIDIA_EMBED_API_KEY,
                rerank_api_key=NVIDIA_RERANK_API_KEY,
                gemini_api_key=GOOGLE_GEMINI_API_KEY
            )

            # --- 3. Load and Chunk Data ---
            print(f"Loading data from {INPUT_FILE}...")
            with open(INPUT_FILE, 'r', encoding='utf-8') as f:
                content = f.read()

            print("Chunking data...")
            chunks = rag_system.chunk_by_table_row(content)
            print(f"Found {len(chunks)} table rows to index.")

            if not chunks:
                print("No chunks found. Exiting.")
            else:
                # --- 4. Index Documents ---
                rag_system.index_documents(chunks, batch_size=10)

                # --- 5. Run the Full Pipeline ---
                query = "how can we use measurement of natural frequency?"

                # This will run retrieve, rerank, and generate
                # It will also save the *retrieval* step results to a JSON
                pipeline_result = rag_system.run_pipeline(
                    query=query,
                    retrieve_top_k=20,  # Get 20 initial results
                    rerank_top_k=5,     # Rerank and keep top 5
                    generate_context_top_k=5, # Use top 5 for context
                    debug_json_path="retrieval_debug_output.json"
                )

                # --- 6. Print Final Answer ---
                print("\n\n--- FINAL GENERATED ANSWER ---")
                print(pipeline_result.get("answer", "No answer generated."))

                # --- 7. Print Top Reranked Chunks (for verification) ---
                print("\n\n--- TOP 3 RERANKED CHUNKS USED FOR CONTEXT ---")
                top_reranked = pipeline_result.get("reranked_results", [])[:3]

                if not top_reranked:
                    print("No reranked results found.")

                for i, chunk in enumerate(top_reranked, 1):
                    print(f"\n{i}. Chunk ID: {chunk['chunk_id']}")
                    print(f"   Rerank Score: {chunk.get('rerank_score', 'N/A'):.4f}")
                    print(f"   Original Similarity: {chunk['similarity_score']:.4f}")
                    print(f"   Text: {chunk['text_preview']}")

        except FileNotFoundError:
            print(f"Error: Input file not found at {INPUT_FILE}")
        except Exception as e:
            print(f"An unexpected error occurred: {e}")